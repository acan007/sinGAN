# logger options
log_iter: 200                  # How often do you want to log the training stats

# optimization options
n_iter: 2000                   # number of training iterations for scale
batch_size: 1                  # batch size
beta1: 0.5                     # Adam parameter
beta2: 0.999                   # Adam parameter
init: kaiming                  # initialization [gaussian/kaiming/xavier/orthogonal]
lr: 0.0005                     # initial learning rate
weight_decay: 0.0001           # weight decay
g_step: 3                      # generator inner steps
d_step: 3                      # discriminator inner steps
w_recon: 10                    # weight for sample reconstruction loss
w_gp: 0.1                      # weight for gradient penalty of Wasserstein adversarial loss

# model options
coarsest_dim: 25               # minimal dimension of coarsest scale
finest_dim: 250                # minimal dimension of finest scale
scale_factor_init: 1.333       # initial scaling factor
gen:
  dim: 32                      # number of filters in the bottommost layer
  norm: bn                     # norm
  activ: lrelu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_layer: 5                   # number of layers in each generator block
dis:
  dim: 32                      # number of filters in the bottommost layer
  norm: bn                     # normalization layer [none/bn/in/ln]
  activ: lrelu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_layer: 5                   # number of layers in each discriminator block

# data options
path_dataset_root: assets/Input
path_input: Images/stone.png                    # path for source data
path_naive: Editing/stone_edit.png              # path for naive data
path_sample_save: results/samples               # path for sample saving
path_model_save: results/models                 # path for model saving
save_name: editing                              # prefix for save path
